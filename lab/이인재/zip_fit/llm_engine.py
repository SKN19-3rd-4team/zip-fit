from typing import Dict, Any
import asyncio
from .models import ChatRequest 
# ğŸŒŸ config.pyì—ì„œ ì„¤ì •ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from .config import settings
# ğŸŒŸ Gongo ì„í¬íŠ¸
from .gongo import Gongo
# ğŸŒŸ OpenAI ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸
from openai import AsyncOpenAI
# ğŸŒŸ ì„¸ì…˜ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from .session import set_session, get_session

class LlmEngine:
    """
    LLM í˜¸ì¶œ, í”„ë¡¬í”„íŠ¸ êµ¬ì„±, LangChain/LangGraph ë“±ì˜ ì§€ëŠ¥í˜• ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    # ğŸŒŸ ìƒì„±ìë¥¼ í†µí•´ Gongo ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì£¼ì…ë°›ìŠµë‹ˆë‹¤.
    def __init__(self, gongo_service: Gongo):
        self.gongo_service = gongo_service
    # ğŸŒŸ ì‹¤ì œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Configì—ì„œ API Key ì‚¬ìš©)
        self.openai_client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        print("âš™ï¸ LlmEngine Initialized with Gongo service.")

    # ----------------------------------------------------
    # ğŸŒŸ ìš”ì²­í•˜ì‹  ë©”ì„œë“œ 1: Gongoì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜¤ëŠ” ë©”ì„œë“œ
    # ----------------------------------------------------
    async def _get_llm_input_text(self, request: ChatRequest) -> str:
        """
        Gongo ì„œë¹„ìŠ¤ì—ì„œ RAG ë° ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ LLM ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        """
        # Gongo ì„œë¹„ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        context_data = await self.gongo_service.get_contextual_data(
            user_id=request.user_id, 
            query=request.user_input
        )
        
        # ìµœì¢…ì ìœ¼ë¡œ LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        llm_input_text = (
            f"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n\n"
            f"{context_data}\n\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: {request.user_input}"
        )
        
        return llm_input_text

    # ----------------------------------------------------
    # ğŸŒŸ ìš”ì²­í•˜ì‹  ë©”ì„œë“œ 2: LLMì„ í˜¸ì¶œí•˜ëŠ” ë©”ì„œë“œ (Mock)
    # ----------------------------------------------------
    async def _call_llm_api(self, prompt_text: str) -> Dict[str, Any]:
        """
        OpenAI, LangChain ë“±ì„ ì´ìš©í•˜ì—¬ ì‹¤ì œ LLM APIë¥¼ í˜¸ì¶œí•˜ê³  ì‘ë‹µì„ ë°›ìŠµë‹ˆë‹¤. (í˜„ì¬ëŠ” Mock)
        """
        
        # ğŸŒŸ ì‹¤ì œ OpenAI LLM í˜¸ì¶œ
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = "ë‹¹ì‹ ì€ ìµœê³ ì˜ ë¶„ì„ê°€ì´ì ì¡°ì–¸ìì…ë‹ˆë‹¤. ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”"
        
        # APIì— ì „ë‹¬í•  ë©”ì„¸ì§€
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt_text}
        ]
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.3
            )
            # ì‘ë‹µ ê²°ê³¼ íŒŒì‹±
            llm_output = response.choices[0].message.content
            
            return {
                "llm_output": llm_output,
                "prompt_used": prompt_text,
                "usage_tokens": response.usage.total_tokens
            }
        
        except Exception as e:
            # ì˜¤ë¥˜ ì²˜ë¦¬
            return {
                "llm_output": f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}",
                "prompt_used": prompt_text,
                "usage_tokens": 0
            }
        
        # # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤.
        # await asyncio.sleep(0.05)
        
        # # Mock ì‘ë‹µì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        # mock_llm_response = {
        #     "llm_output": f"LLMì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt_text)} ë¬¸ì)",
        #     "prompt_used": prompt_text,
        #     "usage_tokens": len(prompt_text) // 5 # ëŒ€ëµì ì¸ í† í° Mock
        # }
        
        # return mock_llm_response


    # async def generate_response(self, request: ChatRequest) -> Dict[str, Any]:
    #     """
    #     Chatting í´ë˜ìŠ¤ì—ì„œ í˜¸ì¶œë˜ëŠ” ë©”ì¸ ì²˜ë¦¬ ë©”ì„œë“œì…ë‹ˆë‹¤.
    #     """
    #     # 1. Gongoë¥¼ í†µí•´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    #     prompt_text = await self._get_llm_input_text(request)
        
    #     # 2. LLM í˜¸ì¶œ
    #     llm_result = await self._call_llm_api(prompt_text)
        
    #     return llm_result
    
    async def generate_response(self, request: ChatRequest) -> Dict[str, Any]:
        
        # ğŸ†• 1. ì„¸ì…˜ í‚¤ ìƒì„±
        session_key = f"messages_userid_{request.user_id}"

        # ğŸ†• 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (íŒŒì¼ì—ì„œ ì½ê¸°)
        history = get_session(session_key, "conversation")
        
        if not history:
            # ê¸°ë¡ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
            history = [{"role": "system", "content": "ë‹¹ì‹ ì€ zip-fit ìƒë‹´ì›ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."}]

        # 3. Gongo ë°ì´í„° ì¡°íšŒ (ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜ Mock ë°ì´í„° í˜¸ì¶œ)
        context_data = await self.gongo_service.get_contextual_data(request.user_id, request.user_input)

        # 4. ì´ë²ˆ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        current_input = (
            f"[ì°¸ê³  ë°ì´í„°]\n{context_data}\n\n"
            f"[ì‚¬ìš©ì ì§ˆë¬¸]: {request.user_input}"
        )
        
        # ğŸ†• 5. íˆìŠ¤í† ë¦¬ì— 'User' ì§ˆë¬¸ ì¶”ê°€
        history.append({"role": "user", "content": current_input})

        try:
            # ğŸ”„ 6. LLM í˜¸ì¶œ (ë‹¨ìˆœ ì§ˆë¬¸ ëŒ€ì‹  'history' ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì „ë‹¬)
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=history, 
                temperature=0.3
            )
            ai_answer = response.choices[0].message.content
            
            # ğŸ†• 7. ë‹µë³€ì„ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•˜ê³  íŒŒì¼ì— ì €ì¥
            history.append({"role": "assistant", "content": ai_answer})
            set_session(session_key, "conversation", history)
            
            return {
                "llm_output": ai_answer,
                "usage_tokens": response.usage.total_tokens
            }
            
        except Exception as e:
            return {"llm_output": f"Error: {str(e)}", "usage_tokens": 0}