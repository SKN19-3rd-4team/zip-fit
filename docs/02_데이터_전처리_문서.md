# 데이터 전처리 문서

## 1. 크롤링 프로세스

### 구현 위치
- **파일**: `lab/이상혁/crw/lh_crw-download-update_func.ipynb`
- **주요 함수**: `crawl_lh_notices_all_data()`, `filter_assign_id_and_save()`

### 크롤링 방법
- **방식**: 웹 크롤링 (POST 방식)
- **대상 URL**: `https://apply.lh.or.kr/lhapply/apply/wt/wrtanc/selectWrtancList.do`
- **HTTP 헤더**: User-Agent, Referer 설정
- **페이지네이션**: 페이지별 50개씩 수집

### 필터링 및 분류
- **게시일 필터**: 2024년 11월 1일 이후 공고만 수집
- **유형별 분류**: 임대/분양 자동 분류
- **지역 필터**: 서울/경기 지역만 선별
- **ID 부여**: `LH_lease_`, `LH_sale_` 접두사 사용

### 데이터 저장
- **형식**: CSV 파일
- **인코딩**: UTF-8-sig
- **정렬**: 게시일, 번호 기준 오름차순

---

## 2. PDF 문서 처리

### 구현 위치
- **도구**: pymupdf4llm
- **처리 과정**: PDF 다운로드 → 텍스트 추출 → 마크다운 변환

### PDF 다운로드
- **소스**: 공고 상세 페이지에서 PDF URL 추출
- **저장 위치**: `announcement_files` 테이블의 `file_path`에 저장
- **파일명**: 공고 ID 기반 명명 규칙 사용

### 텍스트 추출
- **라이브러리**: pymupdf4llm
- **출력 형식**: 마크다운 형식으로 변환
- **구조 보존**: 섹션 헤더, 테이블 구조 유지

---

## 3. 텍스트 청킹 (Chunking)

### 구현 위치
- **파일**: `lab/이인재/규격/chunking.py`
- **클래스**: `SmartChunker`

### 기반 문서
- PDF에서 추출한 텍스트를 마크다운 형식으로 변환한 문서

### 청킹 전략

#### 기본 파라미터
| 파라미터 | 값 | 설명 |
|---------|-----|------|
| 최소 청크 크기 | 100자 | 의미 있는 최소 텍스트 길이 |
| 최적 청크 크기 | 600자 | RAG 검색에 최적화된 크기 |
| 최대 청크 크기 | 1200자 | 단일 청크의 최대 크기 |
| 최대 테이블 크기 | 3000자 | 테이블 청크의 최대 크기 |
| 청크 오버랩 | 150자 | 문맥 보존을 위한 오버랩 |

#### 청킹 방법
- **기본 분할기**: `RecursiveCharacterTextSplitter` 활용
- **구분자 우선순위**: `["\n\n", "\n", " ", ""]`
- **섹션 헤더 감지**: 마크다운 헤더(`#`, `##`), 한글 섹션 표시(`【】`, `■`) 자동 인식
- **테이블 처리**: 테이블은 별도 청크로 분리, 헤더 유지
- **문맥 보존**: 이전 청크의 컨텍스트를 다음 청크에 포함

### 주요 기능

#### 섹션 헤더 감지
- 마크다운 헤더 (`#`, `##`)
- 한글 섹션 표시 (`【】`, `■`)
- 번호 목록 (`1.`, `2.`)

#### 테이블 처리
- 테이블 자동 인식 (파이프(`|`) 기반)
- 큰 테이블 분할 시 헤더 유지
- 테이블 청크는 별도로 관리

#### 의미 필터링
- 최소 단어 수 확인 (5개 이상)
- 빈 청크 제거
- 의미 없는 패턴 제거 (숫자만 있는 경우 등)

---

## 4. 벡터화 (Vectorization)

### 구현 위치
- **파일**: `lab/이인재/규격/vectorizer.py`

### 임베딩 모델
- **모델명**: BAAI/bge-m3
- **차원**: 1024
- **라이브러리**: sentence-transformers

### 벡터화 과정
1. **청크별 임베딩 생성**
   - 각 청크 텍스트를 모델에 입력
   - 정규화된 임베딩 벡터 생성 (`normalize_embeddings=True`)

2. **벡터 저장**
   - PostgreSQL pgvector 확장 사용
   - `document_chunks` 테이블의 `embedding` 컬럼에 저장
   - 벡터 타입: `vector(1024)`

3. **메타데이터 저장**
   - 청크 정보를 JSONB 형식으로 저장
   - 포함 정보:
     - `section`: 섹션 정보
     - `file_name`: 파일명
     - `has_table`: 테이블 포함 여부
     - `chunk_length`: 청크 길이

### 벡터화 진행 상황 추적
- **테이블**: `vectorization_progress`
- **추적 항목**:
  - 벡터화 완료 개수
  - 전체 공고 개수
  - 진행률 (%)

---

## 5. 데이터베이스 저장

### 테이블 구조
- **announcements**: 공고 메타데이터 저장
- **announcement_files**: PDF 파일 정보 저장
- **document_chunks**: 청크 및 임베딩 벡터 저장

### 저장 프로세스
1. CSV 데이터를 `announcements` 테이블에 임포트
2. PDF 다운로드 후 `announcement_files` 테이블에 저장
3. PDF 텍스트 추출 및 청킹
4. 청크별 임베딩 생성 및 `document_chunks` 테이블에 저장
5. 벡터화 완료 표시 (`is_vectorized` 플래그 업데이트)

---

## 6. 전처리 파이프라인

### 전체 프로세스
```
크롤링 → CSV 저장 → PDF 다운로드 → 텍스트 추출 → 청킹 → 벡터화 → DB 저장
```

### 주요 단계
1. **데이터 수집**: 웹 크롤링으로 공고 메타데이터 수집
2. **파일 다운로드**: 공고 상세 페이지에서 PDF 다운로드
3. **텍스트 추출**: PDF를 마크다운 형식으로 변환
4. **청킹**: 문맥 보존 스마트 청킹 수행
5. **벡터화**: 각 청크를 1024차원 벡터로 변환
6. **저장**: PostgreSQL에 메타데이터 및 벡터 저장

---

## 7. 성능 최적화

### 배치 처리
- 청크별 임베딩 생성 시 배치 처리 활용
- 데이터베이스 삽입 시 배치 커밋

### 병렬 처리
- 여러 공고의 PDF 처리 병렬화
- 벡터화 작업 병렬 처리

---

## 8. 품질 관리

### 검증 항목
- 청크 길이 검증 (최소/최대 크기 확인)
- 임베딩 차원 검증 (1024차원 확인)
- 메타데이터 완비 확인

### 오류 처리
- PDF 추출 실패 시 재시도
- 벡터화 실패 시 로깅 및 건너뛰기
- 데이터 무결성 검증

